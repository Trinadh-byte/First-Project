{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trinadh-byte/First-Project/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "iJ3ewARm59id",
        "outputId": "2f2db35e-8fa5-4d3c-9215-f58aa4848427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 26.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=82a37260dc86e9c1b806b39f5a9d2cff4111d038214d903205054c7bb2891739\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate"
      ],
      "metadata": {
        "id": "nvr_a2hn64IG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "from pyspark.sql.functions import regexp_replace"
      ],
      "metadata": {
        "id": "9nPICQr97YxV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "n6madgPR77dl",
        "outputId": "72c9097c-9316-4827-8586-d964fe56ba55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7f889e7c4110>>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) create a raw data frame 'raw_game_df'\n",
        "schema = StructType() \\\n",
        "      .add(\"player\",StringType(),True) \\\n",
        "      .add(\"level\",StringType(),True) \\\n",
        "      .add(\"race\",StringType(),True) \\\n",
        "      .add(\"class\",StringType(),True) \\\n",
        "      .add(\"zone\",StringType(),True) \\\n",
        "      .add(\"guild\",StringType(),True) \\\n",
        "      .add(\"timestamps\",StringType(),True)\n",
        "\n",
        "raw_game_df= spark.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(schema) \\\n",
        "      .load(\"/content/input/game_data.csv\")\n"
      ],
      "metadata": {
        "id": "kTSP53joKHii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tY0txB6yLy2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_game_df.show()"
      ],
      "metadata": {
        "id": "VokNOLXcMC3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.replace Dalaran競技場': 'Dalaran Arena\n",
        "\n",
        "raw_game_df = raw_game_df.withColumn(\"zone\",regxp_replace(\"zone\",\"Dalaran競技場\",\"Dalaran Arena\"))\n",
        "raw_game_df.collect\n",
        "raw_game_df.show()"
      ],
      "metadata": {
        "id": "7AiIEbm_MYps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Write select expression to process the raw data frame with 'Dalaran arena and display player,  level, race, class, zone, guild, timestamp store it to flattened_df \n",
        "\n",
        "raw_game_df.select(\"*\").filter(raw_game_df.zone ==\"Dalaran Arena\").show()\n",
        "flattened_df=raw_game_df"
      ],
      "metadata": {
        "id": "b-JGZhncOgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. write flattened_df in csv format to output directory\n",
        "flattened_df =raw_game_df.write.format(\"csv\").mode(\"overwrite\").save(\"/content/sample_data/output\")\n",
        "flattened_df\n"
      ],
      "metadata": {
        "id": "W_ncfVgZOx24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. create a spark session\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "L3nd78akRRjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9)Create a raw data frame ‘raw_location_df’, read the ‘locations.csv  ’ file . Set the format of file as csv and provide input path.\n",
        "\n",
        "schema_location = StructType() \\\n",
        "      .add(\"Map_ID\",StringType(),True) \\\n",
        "      .add(\"Location_Type\",StringType(),True) \\\n",
        "      .add(\"Location_Name\",StringType(),True) \\\n",
        "      .add(\"Game_Version\",StringType(),True)\n",
        "\n",
        "raw_location_df= spark.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(schema_location) \\\n",
        "      .load(\"/content/input/locations.csv\")\n"
      ],
      "metadata": {
        "id": "Ot7UNoXATNDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. clean the record containing <U+00A0> in location\n",
        "\n",
        "raw_location_df=raw_location_df.where(raw_location_df.Location_Name != regexp_extract(\"Location_Name\",\"^<U+00A0>$\",1) )\n",
        "\n",
        "raw_location_df\n"
      ],
      "metadata": {
        "id": "NqvuW7jTU31O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_location_df.show()"
      ],
      "metadata": {
        "id": "OxkItutIWFWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11.write select expression store it to flattened_location_df\n",
        "\n",
        "flattened_location_df=raw_location_df.select(\"*\").where(raw_location_df.Location_Name != regexp_extract(\"Location_Name\",\"^<U+00A0>$\",1) )\n",
        "flattened_location_df.show()"
      ],
      "metadata": {
        "id": "4Z4bZwrDWbt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12.\tWrite flattened_location_df  in csv format to output directory\n",
        "\n",
        "flattened_location_df=raw_location_df.write.format(\"csv\").mode(\"overwrite\").save(\"/content/output/new\")\n",
        "\n",
        "flattened_location_df\n"
      ],
      "metadata": {
        "id": "cfZDI3gdX7Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. copy \"zones .csv\" to a floder named 'input' in local machine."
      ],
      "metadata": {
        "id": "MBGRYSTuY0Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. create a spark session\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType"
      ],
      "metadata": {
        "id": "UaM8TxbMZdHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "8hOQoFueZ-h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15.Create a raw data frame ‘raw_zones_df’, read the ‘locations.csv  ’ file . Set the format of file as csv and provide input path.\n",
        "\n",
        "schema = StructType() \\\n",
        "      .add(\"Zone_Name\",StringType(),True) \\\n",
        "      .add(\"Continent\",StringType(),True) \\\n",
        "      .add(\"Area\",StringType(),True) \\\n",
        "      .add(\"Zone\",StringType(),True) \\\n",
        "      .add(\"Subzone\",StringType(),True) \\\n",
        "      .add(\"Type\",StringType(),True) \\\n",
        "      .add(\"Size\",StringType(),True) \\\n",
        "      .add(\"Controlled\",StringType(),True) \\\n",
        "      .add(\"Min_rec_level\",StringType(),True) \\\n",
        "      .add(\"Max_rec_level\",StringType(),True) \\\n",
        "      .add(\"Min_bot_level\",StringType(),True) \\\n",
        "      .add(\"Max_bot_level\",StringType(),True)\n",
        "\n",
        "raw_zones_df= spark.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(schema) \\\n",
        "      .load(\"/content/sample_data/input/zones.csv\")\n",
        "      \n",
        " raw_zones_df     \n"
      ],
      "metadata": {
        "id": "FbzGGrgoar1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Clean the records containing  Dalaran<U+7AF6><U+6280><U+5834> to Dalaran Arena\n",
        "\n",
        "\n",
        "raw_zones_df =raw_zones_df.filter(raw_zones_df.Zone_Name != 'Dalaran<U+7AF6><U+6280><U+5834>')\n"
      ],
      "metadata": {
        "id": "vwmPc429cYOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17.Write select expression store it to flattened_zones_df\n",
        "\n",
        "flattened_zones_df = raw_zones_df.select(\"*\").show()"
      ],
      "metadata": {
        "id": "5MB7k1SVc2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Write flattened_zones_df  in csv format to output directory\n",
        "\n",
        "\n",
        "flattened_zone_df=raw_zones_df.write.format(\"csv\").mode(\"overwrite\").save(\"/content/output\")\n",
        "\n",
        "flattened_zone_df\n"
      ],
      "metadata": {
        "id": "KESHWsq5dLaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}